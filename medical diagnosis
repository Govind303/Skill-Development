pip install faiss-cpu langchain sentence-transformers ctransformers beautifulsoup4 requests

import requests
from bs4 import BeautifulSoup
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import CTransformers
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Step 1: Scrape PubMed abstracts
def search_pubmed(query, max_results=5):
    search_url = f"https://pubmed.ncbi.nlm.nih.gov/?term={query.replace(' ', '+')}"
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(search_url, headers=headers)
    soup = BeautifulSoup(res.text, "html.parser")
    links = soup.select("a.docsum-title")
    urls = ["https://pubmed.ncbi.nlm.nih.gov" + link["href"] for link in links[:max_results]]

    abstracts = []
    for url in urls:
        article = requests.get(url, headers=headers)
        article_soup = BeautifulSoup(article.text, "html.parser")
        abstract_tag = article_soup.find("div", class_="abstract-content")
        title_tag = article_soup.find("h1", class_="heading-title")
        if abstract_tag and title_tag:
            text = abstract_tag.get_text(strip=True)
            title = title_tag.get_text(strip=True)
            abstracts.append(Document(page_content=f"{title}\n{text}", metadata={"source": url}))
    return abstracts

# Step 2: Build chain with local embeddings + local LLM
def build_chain(docs):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.split_documents(docs)

    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = FAISS.from_documents(chunks, embeddings)
    retriever = db.as_retriever()

    llm = CTransformers(
        model="TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
        model_file="tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        model_type="llama",
        config={"temperature": 0.6, "max_new_tokens": 256}
    )

    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# Step 3: Run it
if __name__ == "__main__":
    symptoms = input("Enter symptoms or patient case:\n> ")

    print("\n🔍 Searching PubMed...")
    docs = search_pubmed(symptoms)

    if not docs:
        print("❌ No abstracts found.")
    else:
        print(f"✅ Retrieved {len(docs)} abstracts. Building model...\n")
        qa = build_chain(docs)
        result = qa.run(symptoms)
        print("\n📋 Diagnosis Suggestion:\n")
        print(result)
