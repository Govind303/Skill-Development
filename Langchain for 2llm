from langchain.llms import LlamaCpp

# Load Local Model 1 (Mistral)
llm_mistral = LlamaCpp(
    model_path="./models/mistral-7b.Q4_K_M.gguf",
    temperature=0.5,
    max_tokens=512,
    n_ctx=2048,
    verbose=False
)

# Load Local Model 2 (Zephyr or another)
llm_zephyr = LlamaCpp(
    model_path="./models/zephyr-7b.Q4_K_M.gguf",
    temperature=0.7,
    max_tokens=512,
    n_ctx=2048,
    verbose=False
)

# Question to analyze
question = "Explain the ethical concerns around AI in education."

# Get responses from both models
response_mistral = llm_mistral(question)
response_zephyr = llm_zephyr(question)

# Print both outputs
print("ðŸ§  Mistral's Answer:\n", response_mistral)
print("\nðŸ“˜ Zephyr's Answer:\n", response_zephyr)
